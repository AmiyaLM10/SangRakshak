{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KCZBSDo8Pw7",
        "outputId": "685ba4e0-8b4a-4b8e-be61-496bd1751d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9nGsdo08pXX",
        "outputId": "15864d94-e9f4-4483-ee5b-3e565fe76e77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "MBtbzL2rG2S5",
        "outputId": "4822133d-9cb1-4879-ad7b-d278b20bc3c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba97002c-a0e8-461f-b692-d407c97e8a5b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba97002c-a0e8-461f-b692-d407c97e8a5b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training1.py to training1.py\n",
            "Saving training2.py to training2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6vBiCCjG_ei",
        "outputId": "b7b88e4d-3d50-409f-8180-9ae9435552f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdrive  sample_data  training1.py  training2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "\n",
        "def detect_safety(video):\n",
        "    return \"Processing Video for Women's Safety Analysis...\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=detect_safety,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=\"text\",\n",
        "    title=\"Women's Safety AI System\",\n",
        "    description=\"Upload a video to analyze potential threats and ensure safety.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llxvxn6X_Sgt",
        "outputId": "7688a074-2e97-42a7-9970-8cbf720d8cc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure training1.py and training2.py are in the correct path\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    from training1 import GenderClassificationModel, ViolenceDetectionModel  # Import models from training1.py\n",
        "    from training2 import CrowdDetectionModel  # Import model from training2.py\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"❌ Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "# ✅ Load the Gender Classification Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gender_model = GenderClassificationModel().to(device)\n",
        "\n",
        "if os.path.exists(\"gender_classification.pth\"):\n",
        "    gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "    gender_model.eval()\n",
        "else:\n",
        "    print(\"❌ Error: gender_classification.pth not found!\")\n",
        "\n",
        "# ✅ Load the Violence Detection Model\n",
        "violence_model = ViolenceDetectionModel().to(device)\n",
        "if os.path.exists(\"violence_detection_model.pth\"):\n",
        "    violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "    violence_model.eval()\n",
        "else:\n",
        "    print(\"❌ Error: violence_detection_model.pth not found!\")\n",
        "\n",
        "# ✅ Load the Crowd Detection Model\n",
        "crowd_model = CrowdDetectionModel().to(device)\n",
        "if os.path.exists(\"crowd_detection.pth\"):\n",
        "    crowd_model.load_state_dict(torch.load(\"crowd_detection.pth\", map_location=device))\n",
        "    crowd_model.eval()\n",
        "else:\n",
        "    print(\"❌ Error: crowd_detection.pth not found!\")\n",
        "\n",
        "# ✅ Function to process video frames\n",
        "def detect_safety(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    # ✅ Run Gender Classification on the first frame\n",
        "    frame = cv2.resize(frames[0], (128, 128))\n",
        "    frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "    gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "    gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "\n",
        "    # ✅ Run Violence Detection\n",
        "    violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "    violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "\n",
        "    # ✅ Run Crowd Detection\n",
        "    frame = cv2.resize(frames[0], (224, 224))\n",
        "    frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "    crowd_pred = torch.argmax(crowd_model(frame_tensor)).item()\n",
        "    crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "\n",
        "    # ✅ Decision Logic\n",
        "    alert_message = \"✅ Safe Environment\"\n",
        "    if gender_result == \"Female\" and (violence_pred == 1 or crowd_pred == 1):\n",
        "        alert_message = \"🚨 Emergency! Threat Detected!\"\n",
        "\n",
        "    return f\"\"\"\n",
        "    Gender: {gender_result}\n",
        "    Violence: {violence_result}\n",
        "    Crowd Detection: {crowd_result}\n",
        "    Alert: {alert_message}\n",
        "    \"\"\"\n",
        "\n",
        "# ✅ Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=detect_safety,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=\"text\",\n",
        "    title=\"👩‍🦰 Women's Safety AI System\",\n",
        "    description=\"Upload a video to analyze potential threats and ensure safety.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLYsA0HLDWXz",
        "outputId": "a3268a24-f0c1-476e-a717-63ce0212e36b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U07ZTdGlGGX5",
        "outputId": "698a0a00-5525-46f2-dd23-6d5009c83ca9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mohamedmustafa/real-life-violence-situations-dataset/versions/1\n",
            "Subfolders: ['real life violence situations', 'Real Life Violence Dataset']\n",
            "Contents of 'real life violence situations': ['Real Life Violence Dataset']\n",
            "Contents of 'Real Life Violence Dataset': ['Violence', 'NonViolence']\n",
            "real life violence situations contains 1 files.\n",
            "Real Life Violence Dataset contains 2 files.\n",
            "Subfolders: ['Violence', 'NonViolence']\n",
            "Violence: ['V_614.mp4', 'V_415.mp4', 'V_87.mp4', 'V_659.mp4', 'V_504.mp4', 'V_740.mp4', 'V_3.mp4', 'V_351.mp4', 'V_730.mp4', 'V_600.mp4']\n",
            "NonViolence: ['NV_877.avi', 'NV_638.mp4', 'NV_430.mp4', 'NV_926.mp4', 'NV_685.mp4', 'NV_543.mp4', 'NV_719.mp4', 'NV_301.mp4', 'NV_275.mp4', 'NV_908.avi']\n",
            "2025-03-21 15:08:09.108023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742569689.129611   68582 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742569689.136066   68582 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:08:09.157758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-21 15:08:13.093348: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742569693.093569   68582 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "Epoch 1/12\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1742569699.993929   68643 service.cc:148] XLA service 0x1258c9a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1742569699.993984   68643 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2025-03-21 15:08:20.168242: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1742569700.803512   68643 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-03-21 15:08:21.513189: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=0,k4=3,k5=3,k6=3,k7=2} for conv (f16[4,220,220,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,32]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:21.519061: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=3,k4=2,k5=3,k6=3,k7=2} for conv (f16[4,220,220,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,32]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:21.539502: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=4,k6=3,k7=2} for conv (f16[4,220,220,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,32]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:22.258637: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=3,k6=3,k7=2} for conv (f16[4,220,220,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,32]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:22.292414: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=3,k4=2,k5=3,k6=3,k7=2} for conv (f16[4,220,220,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,32]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:22.506607: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,64]{3,2,1,0}), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:22.536803: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=3,k4=2,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,220,220,32]{3,2,1,0}, f16[32,3,3,64]{3,2,1,0}), window={size=3x3}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:25.255425: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[64,3,3,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:25.300566: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=3,k4=2,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[64,3,3,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:27.023430: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng19{k2=2} for conv (f16[64,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[4,222,222,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-03-21 15:08:27.059686: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.036371407s\n",
            "Trying algorithm eng19{k2=2} for conv (f16[64,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[4,222,222,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-03-21 15:08:28.778198: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng19{k2=0} for conv (f16[64,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[4,222,222,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-03-21 15:08:29.160882: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.382749662s\n",
            "Trying algorithm eng19{k2=0} for conv (f16[64,3,3,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[4,222,222,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-03-21 15:08:29.173691: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=4,k4=1,k5=4,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[64,3,3,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:29.197774: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=3,k4=2,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[64,3,3,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "2025-03-21 15:08:29.202189: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:557] Omitted potentially buggy algorithm eng14{k2=0,k4=3,k5=3,k6=3,k7=2} for conv (f16[4,222,222,64]{3,2,1,0}, u8[0]{0}) custom-call(f16[4,222,222,64]{3,2,1,0}, f16[64,3,3,64]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]}\n",
            "I0000 00:00:1742569717.543589   68643 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 219ms/step - accuracy: 0.5298 - loss: 7.0738\n",
            "Epoch 2/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 224ms/step - accuracy: 0.4967 - loss: 8.1045\n",
            "Epoch 3/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 226ms/step - accuracy: 0.5229 - loss: 7.6541\n",
            "Epoch 4/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 225ms/step - accuracy: 0.5279 - loss: 7.5992\n",
            "Epoch 5/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 225ms/step - accuracy: 0.5613 - loss: 7.0562\n",
            "Epoch 6/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 223ms/step - accuracy: 0.5398 - loss: 7.4120\n",
            "Epoch 7/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 225ms/step - accuracy: 0.5341 - loss: 7.4940\n",
            "Epoch 8/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 225ms/step - accuracy: 0.5214 - loss: 7.7023\n",
            "Epoch 9/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 224ms/step - accuracy: 0.5166 - loss: 7.7818\n",
            "Epoch 10/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 224ms/step - accuracy: 0.5419 - loss: 7.3836\n",
            "Epoch 11/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 224ms/step - accuracy: 0.5323 - loss: 7.5169\n",
            "Epoch 12/12\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 222ms/step - accuracy: 0.5141 - loss: 7.8165\n",
            "\u001b[1m487/487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 133ms/step - accuracy: 0.5008 - loss: 7.9935\n",
            "Training Accuracy: 48.46%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 12, in <module>\n",
            "    from training1 import GenderClassificationModel, ViolenceDetectionModel  # Import models from training1.py\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/training1.py\", line 147, in <module>\n",
            "    torch.save(model.state_dict(), model_path)\n",
            "    ^^^^^\n",
            "NameError: name 'torch' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/training1.py\n",
        "import torch  # ✅ Add missing import\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Dense, Flatten, TimeDistributed, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ✅ Ensure model training and saving does not cause issues\n",
        "if __name__ == \"__main__\":\n",
        "    model = nn.Linear(10, 2)  # Dummy model to avoid errors (Replace with actual model)\n",
        "    model_path = \"/content/violence_detection_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"✅ Model saved at {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaOl0hPYHo4s",
        "outputId": "fb4e8851-f4ad-4664-c31c-fcbdbb9d4e73"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/training1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/training1.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igq2cF8EVj6g",
        "outputId": "e86f5114-babf-4193-fbb1-15db9c07b40e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:33:33.600378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571213.621781  125687 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571213.628386  125687 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:33:33.650027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✅ Model saved at /content/violence_detection_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT00Gj7GVnFO",
        "outputId": "139c8221-8853-452b-b834-252268a53dab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:34:00.391322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571240.411896  125820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571240.419833  125820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:34:00.441678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 12, in <module>\n",
            "    from training1 import GenderClassificationModel, ViolenceDetectionModel  # Import models from training1.py\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ImportError: cannot import name 'GenderClassificationModel' from 'training1' (/content/training1.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/training1.py | grep \"class\"\n"
      ],
      "metadata": {
        "id": "ISLZAl1IVuKd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"❌ Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"❌ Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: GenderClassificationModel not found in training1.py\")\n",
        "    gender_model = None\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"❌ Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "    violence_model = None\n",
        "\n",
        "if hasattr(training2, \"CrowdDetectionModel\"):\n",
        "    crowd_model = training2.CrowdDetectionModel().to(device)\n",
        "    if os.path.exists(\"crowd_detection.pth\"):\n",
        "        crowd_model.load_state_dict(torch.load(\"crowd_detection.pth\", map_location=device))\n",
        "        crowd_model.eval()\n",
        "    else:\n",
        "        print(\"❌ Error: crowd_detection.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: CrowdDetectionModel not found in training2.py\")\n",
        "    crowd_model = None\n",
        "\n",
        "\n",
        "def detect_safety(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(frames[0], (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(frames[0], (224, 224))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        crowd_pred = torch.argmax(crowd_model(frame_tensor)).item()\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "    alert_message = \"✅ Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"🚨 Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=detect_safety,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=\"text\",\n",
        "    title=\"👩‍🦰 Women's Safety AI System\",\n",
        "    description=\"Upload a video to analyze potential threats and ensure safety.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA2xVfYgWB3D",
        "outputId": "a8f45008-e063-4980-95a7-acb756dbdbc6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYfrVjXfWHYw",
        "outputId": "efef8376-5b1c-4d93-a0a0-0a6a2a6b8e9f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:35:58.998032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571359.019179  126342 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571359.025357  126342 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:35:59.048115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 13, in <module>\n",
            "    import training2\n",
            "  File \"/content/training2.py\", line 98\n",
            "    !pip install tensorflow numpy matplotlib opencv-python scikit-learn\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/training2.py\n",
        "import torch  # Ensure PyTorch is imported\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define any necessary classes or models\n",
        "class CrowdDetectionModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CrowdDetectionModel, self).__init__()\n",
        "        self.model = tf.keras.applications.MobileNetV2(weights=None, input_shape=(224, 224, 3), classes=2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "# ✅ Instantiate the model\n",
        "crowd_model = CrowdDetectionModel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ktkMdaIWK9h",
        "outputId": "d648345a-5562-4a39-892e-91398125e163"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/training2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/training2.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoWJ31pzWd2K",
        "outputId": "13c4da74-2298-49a7-978f-8f766950168b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:37:13.674448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571433.694856  126692 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571433.701208  126692 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:37:13.722077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-21 15:37:18.540809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742571438.542917  126692 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp8sh20dWeVd",
        "outputId": "9397ff33-13be-4c70-ffd3-1f5173d2172e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:37:44.356582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571464.376981  126834 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571464.383184  126834 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:37:44.404266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-21 15:37:47.703567: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742571467.703737  126834 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "⚠ Warning: GenderClassificationModel not found in training1.py\n",
            "⚠ Warning: ViolenceDetectionModel not found in training1.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 44, in <module>\n",
            "    crowd_model = training2.CrowdDetectionModel().to(device)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'CrowdDetectionModel' object has no attribute 'to'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure training1.py and training2.py are in the correct path\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "gender_model = None\n",
        "violence_model = None\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"❌ Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: GenderClassificationModel not found in training1.py\")\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"❌ Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "\n",
        "\n",
        "crowd_model = None\n",
        "if hasattr(training2, \"CrowdDetectionModel\"):\n",
        "    crowd_model = training2.CrowdDetectionModel()\n",
        "    if os.path.exists(\"crowd_detection.h5\"):\n",
        "        crowd_model.load_weights(\"crowd_detection.h5\")\n",
        "    else:\n",
        "        print(\"❌ Error: crowd_detection.h5 not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: CrowdDetectionModel not found in training2.py\")\n",
        "\n",
        "def detect_safety(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(frames[0], (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(frames[0], (224, 224))\n",
        "        frame_tensor = np.expand_dims(frame, axis=0) / 255.0  # Normalize for TensorFlow\n",
        "        crowd_pred = np.argmax(crowd_model.predict(frame_tensor))\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "\n",
        "    alert_message = \"✅ Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"🚨 Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=detect_safety,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=\"text\",\n",
        "    title=\"👩‍🦰 Women's Safety AI System\",\n",
        "    description=\"Upload a video to analyze potential threats and ensure safety.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgTrnZP9Wz9d",
        "outputId": "c6fee3b5-27c0-4390-ca54-6b7a74d8a86e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aha6m7csWkl_",
        "outputId": "683d4418-d4fc-4ae8-8161-f6f7c0679a93"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:39:02.539920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571542.560337  127215 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571542.566817  127215 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:39:02.588386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-21 15:39:07.544290: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742571547.544510  127215 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "⚠ Warning: GenderClassificationModel not found in training1.py\n",
            "⚠ Warning: ViolenceDetectionModel not found in training1.py\n",
            "❌ Error: crowd_detection.h5 not found!\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://eb398274ae091aa668.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://eb398274ae091aa668.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VB7HCDWW4EJ",
        "outputId": "571f0347-50ae-4b96-f699-3bdde2526e90"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdrive  __pycache__  sample_data  training1.py  training2.py  violence_detection_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "Ua1oRbLGXNZA",
        "outputId": "9779023a-0e19-4d74-bad9-aab29accd198"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-962061f7-23a2-46f1-9e8f-bae70e7e581b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-962061f7-23a2-46f1-9e8f-bae70e7e581b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training2.py to training2 (1).py\n",
            "Saving training1.py to training1 (1).py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rFpR305XeJt",
        "outputId": "a5b61f40-c6cc-4a72-ac13-8546cbf1924a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " app.py   __pycache__  'training1 (1).py'  'training2 (1).py'   violence_detection_model.pth\n",
            " drive\t  sample_data   training1.py\t    training2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dense(2, activation=\"softmax\")  # Binary classification (Crowd / No Crowd)\n",
        "])\n",
        "\n",
        "# ✅ Step 2: Compile the Model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# ✅ Step 3: Create Dummy Training Data (Since We Have No Dataset)\n",
        "X_train = np.random.rand(100, 224, 224, 3)  # 100 random images\n",
        "y_train = np.random.randint(0, 2, 100)  # Random labels (0 = No Crowd, 1 = Crowd)\n",
        "\n",
        "# ✅ Step 4: Train the Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=10)\n",
        "\n",
        "# ✅ Step 5: Save the Model as `crowd_detection.h5`\n",
        "model.save(\"crowd_detection.h5\")\n",
        "print(\"✅ Model saved as crowd_detection.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpidIiYuXjQw",
        "outputId": "57f2bac2-97a8-4c05-b494-3b9a79ac7778"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.5361 - loss: 7.8077\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.4773 - loss: 0.7098\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6467 - loss: 0.6906\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6230 - loss: 0.6799\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6480 - loss: 0.6217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as crowd_detection.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuMsgRsAYGbH",
        "outputId": "20dc3edd-23f5-489e-d0a2-ef544186f995"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " app.py\t\t      __pycache__\t  training1.py\t      violence_detection_model.pth\n",
            " crowd_detection.h5   sample_data\t 'training2 (1).py'\n",
            " drive\t\t     'training1 (1).py'   training2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoUu679sYQX_",
        "outputId": "d0b9c5d6-6673-4382-9171-78ef8151953e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:45:15.203018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742571915.222979  128951 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742571915.229267  128951 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:45:19.849642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742571919.849849  128951 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12798 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "⚠ Warning: GenderClassificationModel not found in training1.py\n",
            "⚠ Warning: ViolenceDetectionModel not found in training1.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 50, in <module>\n",
            "    crowd_model.load_weights(\"crowd_detection.h5\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 357, in load_weights_from_hdf5_group\n",
            "    raise ValueError(\n",
            "ValueError: Layer count mismatch when loading weights from file. Model expected 1 layers, found 4 saved layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from cryptography.fernet import Fernet\n",
        "\n",
        "\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "key = Fernet.generate_key()\n",
        "cipher = Fernet(key)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "gender_model = None\n",
        "violence_model = None\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"⚠ Warning: GenderClassificationModel not found in training1.py\")\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "\n",
        "\n",
        "def build_crowd_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")  # Binary classification (Crowd / No Crowd)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "crowd_model = build_crowd_model()\n",
        "if os.path.exists(\"crowd_detection.h5\"):\n",
        "    crowd_model.load_weights(\"crowd_detection.h5\")\n",
        "else:\n",
        "    print(\"Error: crowd_detection.h5 not found!\")\n",
        "\n",
        "def detect_safety(video_source):\n",
        "    if video_source.startswith(\"rtsp\") or video_source.startswith(\"http\"):\n",
        "        cap = cv2.VideoCapture(video_source)  # Read from CCTV\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_source)  # Read from uploaded file\n",
        "\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "\n",
        "    encrypted_frame = cipher.encrypt(frames[0].tobytes())\n",
        "\n",
        "\n",
        "    decrypted_frame = np.frombuffer(cipher.decrypt(encrypted_frame), dtype=np.uint8).reshape(frames[0].shape)\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(decrypted_frame, (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(decrypted_frame, (224, 224))\n",
        "        frame_tensor = np.expand_dims(frame, axis=0) / 255.0  # Normalize for TensorFlow\n",
        "        crowd_pred = np.argmax(crowd_model.predict(frame_tensor))\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "    alert_message = \"Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=detect_safety,\n",
        "    inputs=gr.Textbox(label=\"CCTV URL (RTSP/HTTP) or Upload Video\"),\n",
        "    outputs=\"text\",\n",
        "    title=\" Women's Safety AI System\",\n",
        "    description=\"Enter a CCTV stream URL or upload a video for real-time safety analysis.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcQT-tN9YTLv",
        "outputId": "ddbbd4db-4fc4-4b2f-a0a8-8fd390217f90"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUj9tisDY1SL",
        "outputId": "9e4796ef-fcf2-4bc9-f8c6-b416041e5d74"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:49:53.342657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742572193.363033  130218 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742572193.369403  130218 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:49:57.779701: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742572197.779882  130218 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12798 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "⚠ Warning: GenderClassificationModel not found in training1.py\n",
            "Warning: ViolenceDetectionModel not found in training1.py\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://4fb3814b00083885f7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2963, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 134, in <module>\n",
            "    iface.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2869, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2967, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1123, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4fb3814b00083885f7.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from cryptography.fernet import Fernet  # Encryption Library\n",
        "\n",
        "# Ensure training1.py and training2.py are in the correct path\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "# Generate Encryption Key (You can store it securely instead)\n",
        "key = Fernet.generate_key()\n",
        "cipher = Fernet(key)\n",
        "\n",
        "# Load Available Models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load PyTorch Models (Gender & Violence Detection)\n",
        "gender_model = None\n",
        "violence_model = None\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: GenderClassificationModel not found in training1.py\")\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "\n",
        "# Define Crowd Detection Model Before Loading Weights\n",
        "def build_crowd_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")  # Binary classification (Crowd / No Crowd)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "crowd_model = build_crowd_model()\n",
        "if os.path.exists(\"crowd_detection.h5\"):\n",
        "    crowd_model.load_weights(\"crowd_detection.h5\")\n",
        "else:\n",
        "    print(\"Error: crowd_detection.h5 not found!\")\n",
        "\n",
        "# Function to process video frames (from CCTV or uploaded video)\n",
        "def detect_safety(video_input, is_url):\n",
        "    if is_url:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from CCTV URL\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from uploaded file\n",
        "\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    # Encrypt the first frame before processing\n",
        "    encrypted_frame = cipher.encrypt(frames[0].tobytes())\n",
        "\n",
        "    # Decrypt for AI processing\n",
        "    decrypted_frame = np.frombuffer(cipher.decrypt(encrypted_frame), dtype=np.uint8).reshape(frames[0].shape)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Run Gender Classification (PyTorch)\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(decrypted_frame, (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "    # Run Violence Detection (PyTorch)\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "    # Run Crowd Detection (TensorFlow)\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(decrypted_frame, (224, 224))\n",
        "        frame_tensor = np.expand_dims(frame, axis=0) / 255.0  # Normalize for TensorFlow\n",
        "        crowd_pred = np.argmax(crowd_model.predict(frame_tensor))\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "    # Decision Logic\n",
        "    alert_message = \"Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "# Gradio Interface with CCTV & Upload Support\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 style=\"text-align: center; font-size: 2.5em; color: #333;\">संरक्षक</h1>\n",
        "        <h2 style=\"text-align: center; font-size: 1.8em; color: #666;\">Together, Let's Redefine Safety. For Women. For Everyone.</h2>\n",
        "        <p style=\"text-align: center; font-size: 1.2em; color: #888;\">\n",
        "        \"A nation’s progress is measured by the status and safety of its women.\" <br> — Jawaharlal Nehru\n",
        "        </p>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## CCTV Live Streaming\")\n",
        "            cctv_url = gr.Textbox(label=\"Enter CCTV Stream URL (RTSP/HTTP)\")\n",
        "            analyze_cctv = gr.Button(\"Analyze CCTV\")\n",
        "            cctv_output = gr.Textbox(label=\"CCTV Analysis Results\")\n",
        "            analyze_cctv.click(fn=detect_safety, inputs=[cctv_url, gr.Boolean(value=True)], outputs=cctv_output)\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## Upload Video\")\n",
        "            uploaded_video = gr.Video(label=\"Upload a Video File\")\n",
        "            analyze_video = gr.Button(\"Analyze Video\")\n",
        "            video_output = gr.Textbox(label=\"Video Analysis Results\")\n",
        "            analyze_video.click(fn=detect_safety, inputs=[uploaded_video, gr.Boolean(value=False)], outputs=video_output)\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <br><br>\n",
        "        <p style=\"text-align: center; font-size: 1.2em; color: #777;\">\n",
        "        Encrypted CCTV & Video Processing for Enhanced Security\n",
        "        </p>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jaTv34ZEki",
        "outputId": "7228446d-8379-492c-9625-b5465e62609f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3wvbuOIbFu2",
        "outputId": "1c491c88-dfa8-42e9-ab1c-adb2cbf54d7d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:57:42.447565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742572662.468759  132263 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742572662.475350  132263 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:57:47.434805: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742572667.435022  132263 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12798 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Warning: GenderClassificationModel not found in training1.py\n",
            "Warning: ViolenceDetectionModel not found in training1.py\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 127, in <module>\n",
            "    gr.Markdown(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\", line 181, in wrapper\n",
            "    return fn(self, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "TypeError: Markdown.__init__() got an unexpected keyword argument 'unsafe_allow_html'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from cryptography.fernet import Fernet  # Encryption Library\n",
        "\n",
        "# Ensure training1.py and training2.py are in the correct path\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "# Generate Encryption Key (You can store it securely instead)\n",
        "key = Fernet.generate_key()\n",
        "cipher = Fernet(key)\n",
        "\n",
        "# Load Available Models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load PyTorch Models (Gender & Violence Detection)\n",
        "gender_model = None\n",
        "violence_model = None\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: GenderClassificationModel not found in training1.py\")\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "\n",
        "# Define Crowd Detection Model Before Loading Weights\n",
        "def build_crowd_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")  # Binary classification (Crowd / No Crowd)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "crowd_model = build_crowd_model()\n",
        "if os.path.exists(\"crowd_detection.h5\"):\n",
        "    crowd_model.load_weights(\"crowd_detection.h5\")\n",
        "else:\n",
        "    print(\"Error: crowd_detection.h5 not found!\")\n",
        "\n",
        "# Function to process video frames (from CCTV or uploaded video)\n",
        "def detect_safety(video_input, is_url):\n",
        "    if is_url:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from CCTV URL\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from uploaded file\n",
        "\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    # Encrypt the first frame before processing\n",
        "    encrypted_frame = cipher.encrypt(frames[0].tobytes())\n",
        "\n",
        "    # Decrypt for AI processing\n",
        "    decrypted_frame = np.frombuffer(cipher.decrypt(encrypted_frame), dtype=np.uint8).reshape(frames[0].shape)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Run Gender Classification (PyTorch)\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(decrypted_frame, (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "    # Run Violence Detection (PyTorch)\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "    # Run Crowd Detection (TensorFlow)\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(decrypted_frame, (224, 224))\n",
        "        frame_tensor = np.expand_dims(frame, axis=0) / 255.0  # Normalize for TensorFlow\n",
        "        crowd_pred = np.argmax(crowd_model.predict(frame_tensor))\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "    # Decision Logic\n",
        "    alert_message = \"Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "# Gradio Interface with CCTV & Upload Support\n",
        "with gr.Blocks() as iface:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <h1 style=\"text-align: center; font-size: 2.5em; color: #333;\">संरक्षक</h1>\n",
        "        <h2 style=\"text-align: center; font-size: 1.8em; color: #666;\">Together, Let's Redefine Safety. For Women. For Everyone.</h2>\n",
        "        <p style=\"text-align: center; font-size: 1.2em; color: #888;\">\n",
        "        \"A nation’s progress is measured by the status and safety of its women.\" <br> — Jawaharlal Nehru\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## CCTV Live Streaming\")\n",
        "            cctv_url = gr.Textbox(label=\"Enter CCTV Stream URL (RTSP/HTTP)\")\n",
        "            analyze_cctv = gr.Button(\"Analyze CCTV\")\n",
        "            cctv_output = gr.Textbox(label=\"CCTV Analysis Results\")\n",
        "            analyze_cctv.click(fn=detect_safety, inputs=[cctv_url, gr.Boolean(value=True)], outputs=cctv_output)\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## Upload Video\")\n",
        "            uploaded_video = gr.Video(label=\"Upload a Video File\")\n",
        "            analyze_video = gr.Button(\"Analyze Video\")\n",
        "            video_output = gr.Textbox(label=\"Video Analysis Results\")\n",
        "            analyze_video.click(fn=detect_safety, inputs=[uploaded_video, gr.Boolean(value=False)], outputs=video_output)\n",
        "\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <br><br>\n",
        "        <p style=\"text-align: center; font-size: 1.2em; color: #777;\">\n",
        "        Encrypted CCTV & Video Processing for Enhanced Security\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjJFT_Lmbeug",
        "outputId": "8d6cafd3-8f8e-4934-8ede-362839e54407"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF56q3sIbfOb",
        "outputId": "1d139510-2c9d-44e0-a800-06c709652b57"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 15:59:25.689803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742572765.710556  132737 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742572765.717035  132737 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 15:59:30.160179: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742572770.160352  132737 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12798 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Warning: GenderClassificationModel not found in training1.py\n",
            "Warning: ViolenceDetectionModel not found in training1.py\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/app.py\", line 143, in <module>\n",
            "    analyze_cctv.click(fn=detect_safety, inputs=[cctv_url, gr.Boolean(value=True)], outputs=cctv_output)\n",
            "                                                           ^^^^^^^^^^\n",
            "AttributeError: module 'gradio' has no attribute 'Boolean'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from cryptography.fernet import Fernet  # Encryption Library\n",
        "\n",
        "# Ensure training1.py and training2.py are in the correct path\n",
        "sys.path.append(\"/content/\")\n",
        "\n",
        "try:\n",
        "    import training1\n",
        "    import training2\n",
        "    from importlib import reload\n",
        "    training1 = reload(training1)\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"Error: training1.py or training2.py not found. Make sure they are uploaded in /content/\")\n",
        "    raise e\n",
        "\n",
        "# Generate Encryption Key (You can store it securely instead)\n",
        "key = Fernet.generate_key()\n",
        "cipher = Fernet(key)\n",
        "\n",
        "# Load Available Models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load PyTorch Models (Gender & Violence Detection)\n",
        "gender_model = None\n",
        "violence_model = None\n",
        "if hasattr(training1, \"GenderClassificationModel\"):\n",
        "    gender_model = training1.GenderClassificationModel().to(device)\n",
        "    if os.path.exists(\"gender_classification.pth\"):\n",
        "        gender_model.load_state_dict(torch.load(\"gender_classification.pth\", map_location=device))\n",
        "        gender_model.eval()\n",
        "    else:\n",
        "        print(\"Error: gender_classification.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: GenderClassificationModel not found in training1.py\")\n",
        "\n",
        "if hasattr(training1, \"ViolenceDetectionModel\"):\n",
        "    violence_model = training1.ViolenceDetectionModel().to(device)\n",
        "    if os.path.exists(\"violence_detection_model.pth\"):\n",
        "        violence_model.load_state_dict(torch.load(\"violence_detection_model.pth\", map_location=device))\n",
        "        violence_model.eval()\n",
        "    else:\n",
        "        print(\"Error: violence_detection_model.pth not found!\")\n",
        "else:\n",
        "    print(\"Warning: ViolenceDetectionModel not found in training1.py\")\n",
        "\n",
        "# Define Crowd Detection Model Before Loading Weights\n",
        "def build_crowd_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")  # Binary classification (Crowd / No Crowd)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "crowd_model = build_crowd_model()\n",
        "if os.path.exists(\"crowd_detection.h5\"):\n",
        "    crowd_model.load_weights(\"crowd_detection.h5\")\n",
        "else:\n",
        "    print(\"Error: crowd_detection.h5 not found!\")\n",
        "\n",
        "# Function to process video frames (from CCTV or uploaded video)\n",
        "def detect_safety(video_input, is_url):\n",
        "    if is_url:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from CCTV URL\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(video_input)  # Read from uploaded file\n",
        "\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return \"Error: No frames found in the video.\"\n",
        "\n",
        "    # Encrypt the first frame before processing\n",
        "    encrypted_frame = cipher.encrypt(frames[0].tobytes())\n",
        "\n",
        "    # Decrypt for AI processing\n",
        "    decrypted_frame = np.frombuffer(cipher.decrypt(encrypted_frame), dtype=np.uint8).reshape(frames[0].shape)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Run Gender Classification (PyTorch)\n",
        "    if gender_model:\n",
        "        frame = cv2.resize(decrypted_frame, (128, 128))\n",
        "        frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "        gender_pred = torch.argmax(gender_model(frame_tensor)).item()\n",
        "        gender_result = \"Female\" if gender_pred == 1 else \"Male\"\n",
        "        results.append(f\"Gender: {gender_result}\")\n",
        "\n",
        "    # Run Violence Detection (PyTorch)\n",
        "    if violence_model:\n",
        "        violence_pred = torch.argmax(violence_model(frame_tensor)).item()\n",
        "        violence_result = \"Violence Detected\" if violence_pred == 1 else \"No Violence\"\n",
        "        results.append(f\"Violence: {violence_result}\")\n",
        "\n",
        "    # Run Crowd Detection (TensorFlow)\n",
        "    if crowd_model:\n",
        "        frame = cv2.resize(decrypted_frame, (224, 224))\n",
        "        frame_tensor = np.expand_dims(frame, axis=0) / 255.0  # Normalize for TensorFlow\n",
        "        crowd_pred = np.argmax(crowd_model.predict(frame_tensor))\n",
        "        crowd_result = \"Crowd Present\" if crowd_pred == 1 else \"No Crowd\"\n",
        "        results.append(f\"Crowd Detection: {crowd_result}\")\n",
        "\n",
        "    # Decision Logic\n",
        "    alert_message = \"Safe Environment\"\n",
        "    if \"Female\" in results and (\"Violence Detected\" in results or \"Crowd Present\" in results):\n",
        "        alert_message = \"Emergency! Threat Detected!\"\n",
        "\n",
        "    return \"\\n\".join(results) + f\"\\nAlert: {alert_message}\"\n",
        "\n",
        "# Gradio Interface with CCTV & Upload Support\n",
        "with gr.Blocks() as iface:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <h1 style=\"text-align: center; font-size: 2.5em; color: #333;\">संरक्षक</h1>\n",
        "        <h2 style=\"text-align: center; font-size: 1.8em; color: #666;\">Together, Let's Redefine Safety. For Women. For Everyone.</h2>\n",
        "        <p style=\"text-align: center; font-size: 1.2em; color: #888;\">\n",
        "        \"A nation’s progress is measured by the status and safety of its women.\" <br> — Jawaharlal Nehru\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## CCTV Live Streaming\")\n",
        "            cctv_url = gr.Textbox(label=\"Enter CCTV Stream URL (RTSP/HTTP)\")\n",
        "            analyze_cctv = gr.Button(\"Analyze CCTV\")\n",
        "            cctv_output = gr.Textbox(label=\"CCTV Analysis Results\")\n",
        "            analyze_cctv.click(fn=detect_safety, inputs=[cctv_url, gr.Checkbox(value=True, visible=False)], outputs=cctv_output)\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"## Upload Video\")\n",
        "            uploaded_video = gr.Video(label=\"Upload a Video File\")\n",
        "            analyze_video = gr.Button(\"Analyze Video\")\n",
        "            video_output = gr.Textbox(label=\"Video Analysis Results\")\n",
        "            analyze_video.click(fn=detect_safety, inputs=[uploaded_video, gr.Checkbox(value=False, visible=False)], outputs=video_output)\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyH1SqVOcdPc",
        "outputId": "c2866d00-5ef0-44e4-d720-4c2fd4acbc47"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2zYo17cceQw",
        "outputId": "b7e34057-3360-4c33-d608-4e43ffba1ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-21 16:03:43.397831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742573023.418682  133883 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742573023.425091  133883 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-21 16:03:48.770361: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1742573028.770577  133883 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12798 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Warning: GenderClassificationModel not found in training1.py\n",
            "Warning: ViolenceDetectionModel not found in training1.py\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://3355981ad566a5b159.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1742573445.878930  133946 service.cc:148] XLA service 0x7a2de0004f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1742573445.878974  133946 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "I0000 00:00:1742573445.912080  133946 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "I0000 00:00:1742573447.028510  133946 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "[ WARN:1@1076.954] global cap.cpp:175 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
            "\n",
            "OpenCV(4.11.0) /io/opencv/modules/videoio/src/cap_images.cpp:293: error: (-215:Assertion failed) !_filename.empty() in function 'open'\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}